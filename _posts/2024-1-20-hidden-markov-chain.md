---
title: "Hidden Markov Chain"
date: 2024-1-20 12:15:00 +0900
excerpt: "Hidden Markov Chain"
header:
  teaser: /assets/images/wallpaper_1.png
  overlay_image: /assets/images/wallpaper_2.png
  overlay_filter: 0.1 # same as adding an opacity of 0.5 to a black background
categories: ai-ml
tags: ai-ml study
use_math: true
classes: wide
---
# Hidden Markov Chain

## Intro

[stanford asr-introduction](https://web.stanford.edu/class/cs224s/syllabus/#week-4-course-project--automatic-speech-recognition-asr-introduction
) ê³¼ [ratsgo's HMM](https://ratsgo.github.io/machine%20learning/2017/03/18/HMMs/) ì„ ë³´ê³  ì¬ì‘ì„±



## Markov Model

$$
P(X_{n} = j | X_{n-1} = i, X_{n-2} = i_{n-2}, ..., X_0 = i_0) = P(X_{n} = j | X_{n-1} = i)
$$

(X_k)ëŠ” ì‹œê°„ (k)ì—ì„œì˜ ìƒíƒœ.

Output-independenceë¥¼ ê°€ì •í•œë‹¤

$$
P(o_t | O_{1}^{t-1}, q_{1}^{t}) = P(o_t | q_t)
$$

í˜„ì¬ ìƒíƒœê°€ ì˜¤ì§ ë°”ë¡œ ì´ì „ì˜ ê³¼ê±° ìƒíƒœì—ë§Œ ì˜ì¡´í•˜ê³  ë” ê³¼ê±°ì˜ ìƒíƒœë‚˜ ì–´ë–»ê²Œ í˜„ì¬ ìƒíƒœì— ë„ë‹¬í–ˆëŠ”ì§€ì— ëŒ€í•œ ì •ë³´ì—ëŠ” ì˜ì¡´í•˜ì§€ ì•ŠëŠ” ëª¨ë¸

ê³¼ê±°ì˜ ì—­ì‚¬ë³´ë‹¤ëŠ” í˜„ì¬ ìƒíƒœê°€ ë¯¸ë˜ì˜ ìƒíƒœë¥¼ ê²°ì •í•˜ëŠ” ë° ë” ì¤‘ìš”í•˜ë‹¤ëŠ” ê°€ì •ì„ ê¸°ë°˜ìœ¼ë¡œ í•¨.

## Markov Chain

`Markov chain = First-order observable Markov Model`

íŠ¹ì • ì‹œì  tì—ì„œì˜ StateëŠ” ë°”ë¡œ ì´ì „ ì‹œì  t-1ì—ì„œì˜ Stateì˜ ì˜í–¥ì„ ë°›ëŠ”ë‹¤ëŠ” ì˜ë¯¸

ë§ˆì½”ë¸Œ ì²´ì¸ì€ ë§ˆì½”ë¸Œ íŠ¹ì„±ì„ ê°–ê³  ì´ì‚°ì ì¸ ì‹œê°„ íë¦„ì— ë”°ë¥¸ ì‹œìŠ¤í…œì˜ ìƒíƒœ ë³€í™”ë¥¼ í™•ë¥ ì ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ì´ì‚°í™•ë¥ ê³¼ì •(ë˜ëŠ” Discrete Markov Process)ì´ë‹¤.

FSM(Finite State Machine)ì²˜ëŸ¼ íŠ¹ì • ì‹œì  tì— ëŒ€í•œ Stateê°€ ì¡´ì¬í•˜ë©°, ì‹œê°„ì˜ ì§„í–‰ì— ë”°ë¼ íŠ¹ì • Stateì—ì„œ ë‹¤ë¥¸ Stateë¡œ Transitionì„ í•œë‹¤

$$
Q = \{q_1, q_2, \ldots, q_N\}; \text{ the state at time } t \text{ is } q_t
$$
Q ëŠ” ê° ì‹œê°„ ë³„ state ë¥¼ ì˜ë¯¸í•œë‹¤.

ì•„ë˜ ì‹ì€ ìŠ¤í…Œì´íŠ¸ë¥¼ ë³€í™˜í•˜ëŠ” ì „ì´ í–‰ë ¬ì´ ëœë‹¤.
$$
a_{ij} = P(q_t = j \mid q_{t-1} = i) \quad 1 \leq i, j \leq N \\
\sum_{j=1}^{N} a_{ij} = 1; \quad 1 \leq i \leq N
$$

### Example

![git](/assets/images/hmm/markov_chain_for_weather.png)

![git](/assets/images/hmm/markov_chain_for_word.png)

### Start state

![git](/assets/images/hmm/markov_model_example.png)

ì‹œì‘ ìƒíƒœ ì—­ì‹œ í™•ë¥ ë¡œ í‘œí˜„í•˜ì—¬ í‘œí˜„í•œë‹¤.

$$
[ \pi_i = P(q_1 = i) \quad 1 \leq i \leq N ]

[ \sum_{i=1}^{N} \pi_i = 1 ]
$$

## Problem

ê¸°ì¡´ ë§ˆì½”ë¸Œ ì²´ì¸ì—ì„œëŠ” ì¶œë ¥ ì‹¬ë³¼ì´ ìƒíƒœ ì‹¬ë³¼ê³¼ ê°™ë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, ë”ìš´ ë‚ ì”¨ë¥¼ ë³´ë©´ ìš°ë¦¬ëŠ” 'ë”ìš´' ìƒíƒœì— ìˆë‹¤ê³  í•  ìˆ˜ ìˆìŒ.

í•˜ì§€ë§Œ ìŒì„± ì¸ì‹ì—ì„œëŠ” ì¶œë ¥ ì‹¬ë³¼ì€ ìŒí–¥ ë²¡í„°(Cepstrum: í‘¸ë¦¬ì— ë³€í™˜í•œ íŠ¹ì§•), ìˆ¨ê²¨ì§„ ìƒíƒœëŠ” 'phones'(ìŒì†Œ)ì´ë‹¤.

ê·¸ë˜ì„œ ë§ˆì½”ë¸Œ ì²´ì¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ëª»í•œë‹¤. ê·¸ë˜ì„œ ì…ë ¥ ì‹¬ë³¼ê³¼ ìƒíƒœê°€ ê°™ì§€ ì•Šì€ ë§ˆì½”ë¸Œ ì²´ì¸ì˜ í™•ì¥ì¸ 

íˆë“  ë§ˆì½”ë¸Œ ëª¨ë¸(Hidden Markov Model)ì„ ì‚¬ìš©í•œë‹¤.

ìš°ë¦¬ê°€ ì–´ë–¤ ìƒíƒœì— ìˆëŠ”ì§€ ì•Œ ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸.

## Hidden Markov Chain

ê´€ì¸¡ì¹˜ ë’¤ì— ì€ë‹‰ë˜ì–´ ìˆëŠ” ìƒíƒœ(state)ë¥¼ ì¶”ì •í•˜ê³ ì í•  ë•Œ ì‚¬ìš©

$$
P(q_i | q_1, \ldots, q_{i-1}) = P(q_i | q_{i-1})
$$

### Hidden Markov Chain Case

ì•„ë˜ëŠ” ëŒ€í‘œì ì¸ HMMì´ ì‚¬ìš©ë˜ëŠ” ì¼€ì´ìŠ¤ 

- ë‹¹ì‹ ì€ 2799ë…„ì˜ ê¸°í›„í•™ìì…ë‹ˆë‹¤. 
- ì§€êµ¬ ì˜¨ë‚œí™”ë¥¼ ì—°êµ¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. 
- 2008ë…„ ì—¬ë¦„, ë©”ë¦´ëœë“œì£¼ ë³¼í‹°ëª¨ì–´ì˜ ë‚ ì”¨ ê¸°ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 
- í•˜ì§€ë§Œ ì œì´ìŠ¨ ì•„ì´ìŠ¤ë„ˆì˜ ì¼ê¸°ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. 
- ê·¸ ì¼ê¸°ì—ëŠ” ê·¸ ì—¬ë¦„, ì œì´ìŠ¨ì´ ë§¤ì¼ ëª‡ ê°œì˜ ì•„ì´ìŠ¤í¬ë¦¼ì„ ë¨¹ì—ˆëŠ”ì§€ê°€ ê¸°ë¡ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 
- ìš°ë¦¬ì˜ ì„ë¬´: ê·¸ ì—¬ë¦„ ì–¼ë§ˆë‚˜ ë”ì› ëŠ”ì§€ ì•Œì•„ë‚´ëŠ” ê²ƒì…ë‹ˆë‹¤.

![git](/assets/images/hmm/hmm_example.png)

ğµëŠ” ë°©ì¶œí™•ë¥ (emission probablity, likly hood), ì€ë‹‰ëœ ìƒíƒœë¡œë¶€í„° ê´€ì¸¡ì¹˜ê°€ íŠ€ì–´ë‚˜ì˜¬ í™•ë¥ ì´ë¼ëŠ” ì˜ë¯¸

![git](/assets/images/hmm/emission_example.png)

$$
P(3 1 3, \text{hot hot cold}) = P(\text{hot} | \text{start}) \times P(\text{hot} | \text{hot}) \times P(\text{cold} | \text{hot}) \times P(3 | \text{hot}) \times P(1 | \text{hot}) \times P(3 | \text{cold}) = 0.8 \times 0.6 \times 0.3 \times 0.4 \times 0.2 \times 0.1
$$

![git](/assets/images/hmm/notation.png)

## The Three Basic Problems for HMMs 
1. Evaluation: ê´€ì¸¡ ì‹œí€€ìŠ¤ O=(o1o2â€¦oT)ì™€ HMM ëª¨ë¸ F = (A,B)ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ëª¨ë¸ì„ ê³ ë ¤í•œ ê´€ì¸¡ ì‹œí€€ìŠ¤ì˜ í™•ë¥  P(O|F)ë¥¼ ì–´ë–»ê²Œ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆì„ê¹Œ?
2. Decoding: ê´€ì¸¡ ì‹œí€€ìŠ¤ O=(o1o2â€¦oT)ì™€ HMM ëª¨ë¸ F = (A,B)ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì–´ë–¤ ì˜ë¯¸ì—ì„œ ìµœì ì¸ ìƒíƒœ ì‹œí€€ìŠ¤ Q=(q1q2â€¦qT)ë¥¼ ì–´ë–»ê²Œ ì„ íƒí•  ìˆ˜ ìˆì„ê¹Œ? (ì¦‰, ê´€ì¸¡ê°’ì„ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” ì‹œí€€ìŠ¤ëŠ” ë¬´ì—‡ì¸ê°€?)
3. Learning: ëª¨ë¸ íŒŒë¼ë¯¸í„° F = (A,B)ë¥¼ ì–´ë–»ê²Œ ì¡°ì •í•´ì•¼ P(O| F)ë¥¼ ìµœëŒ€í™”í•  ìˆ˜ ìˆì„ê¹Œ?

## Evaluation : Forward Algorithm

emission probablity ë¥¼ ëª¨ë“  ê²½ìš°ì— ëŒ€í•´ì„œ êµ¬í•˜ë ¤ë©´ ë§ì€ ê²½ìš°ì— ìˆ˜ê°€ ì¡´ì¬í•œë‹¤. ì´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•˜ê¸° ìœ„í•´ Dynamic Programming ì„ ì‚¬ìš©í•œë‹¤.

$$
\alpha_t(j) = \sum_{i=1}^{n} \alpha_{t-1}(i) \times a_{ij} \times b_j(o_t)
$$

$alpha_t(1)$ , $alpha_t(2)$ ë“±ì„ memoization í•˜ì—¬ ë°˜ë³µí•˜ì—¬ ì‚¬ìš©

## Decoding : Viterbi intuition

ê°€ì¥ íš¨ìœ¨ì´ ì¢‹ì€ ì‹œí€€ìŠ¤ë¥¼ ë½‘ëŠ” ì•Œê³ ë¦¬ì¦˜

$$
v_t(j) = \max_{i \in \{1, \ldots, n\}} \left[ v_{t-1}(i) \times a_{ij} \times b_j(o_t) \right]
$$

Forward Algorithmì€ ê° ìƒíƒœì—ì„œì˜ Î±ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ ê°€ëŠ¥í•œ ëª¨ë“  ê²½ìš°ì˜ ìˆ˜ë¥¼ ê³ ë ¤í•´ ê·¸ í™•ë¥ ë“¤ì„ ë”í•´ì¤¬ë‹¤ë©´(sum), ë””ì½”ë”©ì€ ê·¸ í™•ë¥ ë“¤ ê°€ìš´ë° ìµœëŒ€ê°’(max)ì— ê´€ì‹¬ì´ ìˆë‹¤.

ë¹„í„°ë¹„ ì—­ì‹œ ê³„ì‚° ê²°ê³¼ë¥¼ í™œìš©í•˜ëŠ” ë‹¤ì´ë‚´ë¯¹ í”„ë¡œê·¸ë˜ë° ê¸°ë²•ì„ ì“´ë‹¤.

![git](/assets/images/hmm/viterbi_example.png)

ì‹œí€€ìŠ¤ë¥¼ í™•ì¸í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ìµœëŒ€í™•ë¥ ì„ êµ¬í•œ ë’¤ backtracking ê³¼ì •ì„ ê±°ì¹œë‹¤. íŒŒë€ìƒ‰ ì„ ì´ backtracking ê³¼ì •

## Learning

ì „ë°©í™•ë¥  : í˜„ì¬ ìƒíƒœì˜ ë°œìƒ í™•ë¥ 

í›„ë°©í™•ë¥  : í˜„ì¬ ìƒíƒœ ì´í›„ì˜ ë°œìƒí™•ë¥ 

$$
\alpha_t(j) = \sum_{i=1}^{n} \alpha_{t-1}(i) \times a_{ij} \times b_j(o_t)
\beta_t(i) = \sum_{j=1}^{n} a_{ij} \times b_j(o_{t+1}) \times \beta_{t+1}(j)
$$

![git](/assets/images/hmm/back_forward.png)

ìƒíƒœì˜ ë°œìƒ ìš°ë„(likelihood) : ì „ë°©í™•ë¥  * í›„ë°©í™•ë¥ 

$$
\alpha_t(j) \times \beta_t(j) = P(q_t = j, \mathbf{O} \mid \lambda)
$$

t ë²ˆì§¸ ì‹œì ì— ìƒíƒœ j ì¼ í™•ë¥ ì„ ì˜ë¯¸í•œë‹¤.

### Emission Calcualte

ì‹œì ì— ğ‘—ë²ˆì§¸ ìƒíƒœì¼ í™•ë¥  Î³ğ‘¡(ğ‘—)

$$
\gamma_t(j) = \frac{P(q_t = j, \mathbf{O} \mid \lambda)}{P(\mathbf{O} \mid \lambda)} = \frac{\alpha_t(j) \times \beta_t(j)}{\sum_{s=1}^{n} \alpha_t(s) \times \beta_t(s)}
$$

ğ‘— ë²ˆì§¸ ìƒíƒœì—ì„œ ê´€ì¸¡ì¹˜ ğ‘£ğ‘˜ê°€ ë‚˜íƒ€ë‚  ë°©ì¶œí™•ë¥ 

$$
\hat{b}_j(v_k) = \frac{\sum_{t=1}^{T} \text{s.t. } o_t = v_k \gamma_t(j)}{\sum_{t=1}^{T} \gamma_t(j)}
$$

ë°©ì¶œí™•ë¥  ğ‘Ì‚ ğ‘—(ğ‘£ğ‘˜)ì˜ ë¶„ëª¨ëŠ” ğ‘— ë²ˆì§¸ ìƒíƒœê°€ ë‚˜íƒ€ë‚  í™•ë¥ 

ë¶„ìëŠ” ğ‘—ë²ˆì§¸ ìƒíƒœì´ë©´ì„œ ê·¸ ë•Œ ê´€ì¸¡ì¹˜(ğ‘œğ‘¡)ê°€ ğ‘£ğ‘˜ì¼ í™•ë¥ 

`ì–´ë–¤ ì‹œì ì´ë“  ğ‘—ë²ˆì§¸ ìƒíƒœê°€ ë‚˜íƒ€ë‚  í™•ë¥  Î³ê°€ ì¡´ì¬í•˜ë¯€ë¡œ ğ‘‡ê°œ ê´€ì¸¡ì¹˜ ì‹œí€€ìŠ¤ ì „ì²´ì— ê±¸ì³ ëª¨ë“  ì‹œì ì— ëŒ€í•´ Î³ë¥¼ ë”í•´ì£¼ëŠ” ê²ƒ`

### Transition Probability Update And Î¾

ğ‘¡ ì‹œì ì— ğ‘– ë²ˆì§¸ ìƒíƒœì´ê³  ğ‘¡+1 ì‹œì ì— ğ‘— ë²ˆì§¸ `ìƒíƒœ`ì¼ í™•ë¥  Î¾

$$
\xi_t(i, j) = \frac{P(q_t = i, q_{t+1} = j, \mathbf{O} \mid \lambda)}{P(\mathbf{O} \mid \lambda)}
$$

`ì „ì´`í•  í™•ë¥ 

$$
\xi_t(i, j) = \frac{P(q_t = i, q_{t+1} = j, \mathbf{O} \mid \lambda)}{P(\mathbf{O} \mid \lambda)} = \frac{\alpha_t(i) \times a_{ij} \times b_j(o_{t+1}) \times \beta_{t+1}(j)}{\sum_{s=1}^{n} \alpha_t(s) \times \beta_t(s)}
$$


ë¶„ëª¨ : ëª¨ë“  `ìƒíƒœ`ê°€ ë  í™•ë¥  

ë¶„ì : íŠ¹ì • `ìƒíƒœ`ê°€ ë  í™•ë¥ 

$$
\hat{a}_{ij} = \frac{\sum_{t=1}^{T-1} \xi_t(i, j)}{\sum_{t=1}^{T-1} \sum_{k=1}^{N} \xi_t(i, k)}
$$

![git](/assets/images/hmm/transition.png)

### Learning : EM Algorithm

ì€ë‹‰ë§ˆì½”í”„ëª¨ë¸ì˜ íŒŒë¼ë©”í„°ëŠ” ì „ì´í™•ë¥ (Transition) ğ´ ì™€ ë°©ì¶œí™•ë¥ (Emission) ğµ

ì´ ë‘ íŒŒë¼ë©”í„°ë¥¼ ë™ì‹œì— ì¶”ì •í•˜ê¸°ëŠ” ì–´ë µë‹¤. 

ì˜ˆì œì—ì„œ ìš°ë¦¬ê°€ ê´€ì¸¡ê°€ëŠ¥í•œ ê²ƒì€ ì•„ì´ìŠ¤í¬ë¦¼ì˜ ê°œìˆ˜ë¿, ê¶ê·¹ì ìœ¼ë¡œ ì•Œê³  ì‹¶ì€ ë‚ ì”¨ëŠ” ìˆ¨ê²¨ì ¸ ìˆë‹¤.

ë”°ë¼ì„œ HOTì—ì„œ COLDë¡œ ì „ì´í•  í™•ë¥ ì€ ë¬¼ë¡  ë‚ ì”¨ê°€ ë”ìš¸ ë•Œ ì•„ì´ìŠ¤í¬ë¦¼ì„ 1ê°œ ë¨¹ì„ ë°©ì¶œí™•ë¥  ë”°ìœ„ë¥¼ ëª¨ë‘ í•œë²ˆì— ì•Œ ìˆ˜ ì—†ë‹¤. 

ì´ëŸ´ ë•Œ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ê²ƒì´ EM ì•Œê³ ë¦¬ì¦˜. HMMì—ì„œëŠ” ì´ë¥¼ â€˜ë°”ì›€-ì›°ì¹˜ ì•Œê³ ë¦¬ì¦˜â€™ ë˜ëŠ” â€˜Forward, Backward Algorithmâ€™ì´ë¼ê³ ë„ ë¶€ë¥¸ë‹¤.

### E-step

ëª¨ë¸ íŒŒë¼ë©”í„° Î», ì¦‰ ì „ì´í™•ë¥  ğ´, ë°©ì¶œí™•ë¥  ğµ ë¥¼ ê³ ì •ì‹œí‚¨ ìƒíƒœì—ì„œ ê´€ì¸¡ì¹˜ ğ‘‚ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë°©í™•ë¥  Î± ì™€ í›„ë°©í™•ë¥  Î²ë¥¼ ì—…ë°ì´íŠ¸.
ì´ Î± ì™€ Î² ë¥¼ ë°”íƒ•ìœ¼ë¡œ Î³ ì™€ Î¾ ë¥¼ ê°ê° ê³„ì‚°.

### M-step

E-stepì—ì„œ êµ¬í•œ Î³ ì™€ Î¶ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ íŒŒë¼ë©”í„° ğ´, ğµë¥¼ ì—…ë°ì´íŠ¸

![git](/assets/images/hmm/pseudo.png)

### Programming

[ratsgo's HMM](https://ratsgo.github.io/machine%20learning/2017/10/14/computeHMMs/) ì°¸ê³ 
